/* 
 * AWS Lambda Fan-Out Utility
 * 
 * Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 * 
 * Licensed under the Apache License, Version 2.0 (the "License").
 * You may not use this file except in compliance with the License.
 * A copy of the License is located at
 * 
 *  http://aws.amazon.com/apache2.0
 * 
 * or in the "license" file accompanying this file. This file is distributed
 * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing
 * permissions and limitations under the License.
 * 
 */

/* 
 * This AWS Lambda Node.js function receives records from an Amazon Kinesis Stream
 *  or an Amazon DynamoDB Stream, and sends them to other endpoints as defined in
 *  its configuration table (check configuration.js for details).
 */

// Modules
var transformation = require('./lib/transformation.js');
var configuration = require('./lib/configuration.js');
var statistics = require('./lib/statistics.js');
var services = require('./lib/services.js');
var async = require('async');

// Service configuration
var config = {
	parallelTargets    : 2,    // Number of parallel targets for fan-out destination
	parallelPosters    : 2,    // Number of parallel posters for fan-out destination
	debug              : false // Activate debug messages
};
configuration.configure(config);
statistics.configure(config);
services.configure(config);

//********
// This function posts data to the specified service
//  If the target is marked as 'collapse', records will
//  be grouped in a single payload before being sent
function postToService(serviceReference, target, records, stats, callback) {
	var parallelPosters = target.parallel ? config.parallelPosters : 1;
	var errors = [];
  var definition = serviceReference.definition;
  var service = serviceReference.service;
  var limits = definition.limits;

	var maxRecords = limits.maxRecords;
	var maxSize = limits.maxSize;
	var maxUnitSize = limits.maxUnitSize;
  var includeKey = limits.includeKey;
	var listOverhead = limits.listOverhead;
	var recordOverhead = limits.recordOverhead;
	var interRecordOverhead = limits.interRecordOverhead;

  // Filter invalid records
	records = records.filter(function (record) {
    var size = record.size + (includeKey ? Buffer.byteLength(record.key) : 0);
		if((size + listOverhead + recordOverhead) > maxUnitSize) {
			console.error("Record too large to be pushed to target '" + target.id + "' of type '" + target.type + "':\n", JSON.stringify(record));
			errors.push(new Error("Record too large, was removed"));
			return false;
		} else {
			return true;
		}
	});

	// Group records per block for sending
	var maxRecordsPerBlock = (target.collapse !== null) && (target.collapse != "") && (target.collapse != "none") ? maxRecords : 1;
	var blocks = [];
	var blockSize = listOverhead;
	var block = [];
	while(records.length > 0) {
		var record = records.shift();
		var recordSize = record.size + (includeKey ? record.key.length : 0) + recordOverhead + (block.length > 0 ? interRecordOverhead: 0);

		if(((blockSize + recordSize) > maxSize) || (block.length >= maxRecordsPerBlock)) {
			// Block full, start a new block
			blocks.push(block);
			block = [];
			blockSize = listOverhead;
		}

		// Add the record to the records to send
		blockSize = blockSize + recordSize;
		block.push(record);
	}
	if(block.length > 0) {
		blocks.push(block);
		block = [];
	}

	// Posts the blocks to the target services
  var queue = async.queue(function(block, done) {
    definition.send(service, target, block.records, done);
  }, parallelPosters);

  queue.drain = function() {
    serviceReference.dispose();
    callback((errors.length > 0) ? new Error("An error occured while pushing data to an AWS Service"): null);
  };

  // Add all targets to the queue
  blocks.forEach(function(block) {
    queue.push({ records: block }, function(err) {
      if(err) {
        errors.push(err);
        console.error("An error occured while pushing data to target '" + target.id + "' of type '" + target.type + "':", err);
      }
    });
  });
}

//********
// This function transfers an entire event to the underlying service
function interceptService(serviceReference, target, event, stats, callback) {
  serviceReference.definition.intercept(serviceReference.service, target, event, function(err) {
    serviceReference.dispose();
    callback(err);
  });
}

//********
// This function manages the messages for a target
function sendMessages(eventSourceARN, target, event, stats, callback) {
  if(config.debug) {
    console.log("Processing target '" + target.id + "'");
  }

  var start = Date.now();
  stats.addTick('targets#' + eventSourceARN);
  stats.register('records#' + eventSourceARN + '#' + target.destination, 'Records', 'stats', 'Count', eventSourceARN, target.destination);
  stats.addValue('records#' + eventSourceARN + '#' + target.destination, event.Records.length);

  async.waterfall([
      function(done) { services.get(target, done); },
      function(serviceReference, done) { 
        var definition = serviceReference.definition;
        if(definition.intercept) {
          if(target.passthrough) {
            transformation.transformRecords(event.Records, target, function(err, transformedRecords) {
              transformedRecords.forEach(function(record) { record.data = record.data.toString('base64') });
              interceptService(serviceReference, target, { Records: transformedRecords }, stats, done);
            });
          } else {
            interceptService(serviceReference, target, event, stats, done);
          }
        } else if (definition.send) {
          transformation.transformRecords(event.Records, target, function(err, transformedRecords) {
            postToService(serviceReference, target, transformedRecords, stats, done);
          });
        } else {
          done(new Error("Invalid module '" + target.type + "', it must export either an 'intercept' or a 'send' method"));
        }
      }
    ], function(err) {
      if(err) {
        console.error("Error while processing target '" + target.id + "': " + err);
        callback(new Error("Error while processing target '" + target.id + "': " + err));
        return;
      }
      var end = Date.now();
      var duration = Math.floor((end - start) / 10) / 100;
      if(config.debug) {
        console.log("Target '" + target.id + "' for source '" + eventSourceARN + "' successfully processed in" , duration, "seconds with", event.Records.length,"records");
      }
      callback();
    });
}

//********
// This function reads a set of records from Amazon Kinesis or Amazon DynamoDB Streams and sends it to all subscribed parties
function fanOut(eventSourceARN, event, context, targets, stats, callback) {
  if(targets.length === 0) {
    console.log("No output subscribers found for this event");
    callback(null);
    return;
  }

  var start        = Date.now();
  var hasErrors    = false;

  var queue = async.queue(function(target, done) {
    sendMessages(eventSourceARN, target, event, stats, done);
  }, config.parallelTargets);

  queue.drain = function() {
    var end = Date.now();
    var duration = Math.floor((end - start) / 10) / 100;
    if(hasErrors) {
      console.error("Processing of subscribers for this event ended with errors, check the logs in" , duration, "seconds");
      callback(new Error("Some processing errors occured, check logs"));
    } else {
      console.log("Processing succeeded, processed " + event.Records.length + " records for " + targets.length + " targets in" , duration, "seconds");
      callback(null);
    }
  };

  // Add all targets to the queue
  targets.forEach(function(target) {
    queue.push(target, function(err) {
      if(err) {
        console.error("Error processing record: ", err);
        hasErrors = true;
      }
    });
  });
}

//********
// Lambda entry point. Loads the configuration and does the fanOut
exports.handler = function(event, context) {
  var stats = statistics.create();
  stats.register('sources', 'Sources', 'counter', 'Count'); // source, destination
  stats.register('records', 'Records', 'counter', 'Count'); // source, destination

  if (config.debug) {
    console.log("Starting process of " + event.Records.length + " events");
  }

  // Group records per source ARN
  var sources = {};
  event.Records.forEach(function(record) {
    var eventSourceARN = record.eventSourceARN ||Â record.TopicArn;
    if(! sources.hasOwnProperty(eventSourceARN)) {
      stats.addTick('sources');
      stats.register('records#' + eventSourceARN, 'Records', 'counter', 'Count', eventSourceARN);
      stats.register('targets#' + eventSourceARN, 'Targets', 'counter', 'Count', eventSourceARN);
      sources[eventSourceARN] = { Records: [record] };
    } else {
      sources[eventSourceARN].Records.push(record);
    }
    stats.addTick('records#' + eventSourceARN);
  });

  var eventSourceARNs = Object.keys(sources);
  var hasError = false;

  var queue = async.queue(function(eventSourceARN, callback) {
    async.waterfall([
        function(done) {  configuration.get(eventSourceARN, services.definitions, done); },
        function(targets, done) {  fanOut(eventSourceARN, sources[eventSourceARN], context, targets, stats, done); }
      ],
      callback);
  });

  queue.drain = function() {
    stats.publish(function() {
      if(hasError) {
        context.fail('Some processing errors occured, check logs'); // ERROR with message
      } else {
        context.succeed("Done processing all subscribers for this event, no errors detected"); // SUCCESS with message
      }
    });
  };

  eventSourceARNs.forEach(function(eventSourceARN) {
    queue.push(eventSourceARN, function(err) {
      if(err) {
        console.error("Error while processing events from source '" + eventSourceARN + "'", err);
        hasError = true;
      }
    })
  });
};
